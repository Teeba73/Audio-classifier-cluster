{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM09Vn6Vy1gGZXUQ1hcPDSb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Teeba73/Audio-classifier-cluster/blob/main/Audio1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install webrtcvad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9bbEhs-PAtD",
        "outputId": "59d70ad7-bf70-4d32-b73c-6f5036ee4b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting webrtcvad\n",
            "  Downloading webrtcvad-2.0.10.tar.gz (66 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/66.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m61.4/66.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: webrtcvad\n",
            "  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp311-cp311-linux_x86_64.whl size=73498 sha256=a343b48dc6e53c366e3879d4c8a72f34b9dfd9002cdc7e33dfad8735eb8e9795\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/65/3f/292d0b656be33d1c801831201c74b5f68f41a2ae465ff2ee2f\n",
            "Successfully built webrtcvad\n",
            "Installing collected packages: webrtcvad\n",
            "Successfully installed webrtcvad-2.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1DfCnWiHa4O"
      },
      "outputs": [],
      "source": [
        "import os  # للتعامل مع الملفات والمجلدات\n",
        "import tarfile  # لفك ضغط الملفات\n",
        "import librosa  # لتحليل الصوت واستخراج الميزات\n",
        "import numpy as np  # للحسابات الرياضية\n",
        "import pandas as pd  # لمعالجة البيانات\n",
        "import webrtcvad  # لكشف الصوت البشري (إزالة الضوضاء)\n",
        "from tqdm import tqdm  # لعرض شريط التقدم\n",
        "from sklearn.model_selection import train_test_split  # لتقسيم البيانات\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # لتقييم النموذج\n",
        "import matplotlib.pyplot as plt  # لرسم الرسوم البيانية\n",
        "import soundfile as sf  # لقراءة الملفات الصوتية\n",
        "from xgboost import XGBClassifier  # خوارزمية التصنيف\n",
        "from sklearn.preprocessing import StandardScaler  # لتوحيد البيانات\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score\n",
        ")#Classification Matrics\n",
        "\n",
        "# Configuration\n",
        "TAR_PATH = \"/content/cv-corpus-19.0-delta-2024-09-13-en.tar.gz\"  # مسار الملف المضغوط\n",
        "EXTRACT_TO = \"/content/cv-corpus-19.0-delta-2024-09-13\"  # مجلد فك الضغط\n",
        "SNR_THRESHOLD = 15  # الحد الفاصل بين الصوت النظيف والضجيج (dB)\n",
        "vad = webrtcvad.Vad(2)  # كاشف الصوت البشري (مستوى العدوانية: 0-3)\n",
        "\n",
        "# Create output directories\n",
        "#os.makedirs(\"train\", exist_ok=True)\n",
        "#os.makedirs(\"test\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_dataset():\n",
        "    if not os.path.exists(TAR_PATH):\n",
        "        print(f\"Error: Dataset file not found at {TAR_PATH}\")\n",
        "        # You might want to raise an exception or exit here\n",
        "        return\n",
        "    # Optional: Add a check for file size if you know the expected size\n",
        "    # import os\n",
        "    # expected_size_bytes = ... # Get the expected size from the dataset source\n",
        "    # if os.path.getsize(TAR_PATH) < expected_size_bytes:\n",
        "    #     print(f\"Error: Dataset file is likely incomplete. Size is {os.path.getsize(TAR_PATH)} bytes, expected at least {expected_size_bytes} bytes.\")\n",
        "    #     return\n",
        "\n",
        "    if not os.path.exists(EXTRACT_TO):  # إذا لم يتم فك الضغط من قبل\n",
        "        print(\"Extracting dataset...\")\n",
        "        try:\n",
        "            with tarfile.open(TAR_PATH, \"r:gz\") as tar:\n",
        "                tar.extractall(path=os.path.dirname(EXTRACT_TO))  # فك الضغط\n",
        "            print(\"Extraction complete.\")\n",
        "        except tarfile.ReadError as e:\n",
        "            print(f\"Error reading tar file: {e}\")\n",
        "            print(\"The dataset file might be corrupted or incomplete. Please re-download it.\")\n",
        "        except EOFError as e:\n",
        "            print(f\"EOFError during extraction: {e}\")\n",
        "            print(\"The dataset file ended unexpectedly. It might be incomplete or corrupted. Please re-download it.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during extraction: {e}\")\n",
        "    else:\n",
        "        print(\"Dataset already extracted\")  # إذا كان موجودًا مسبقًا"
      ],
      "metadata": {
        "id": "hKH0OTozHm8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_mfcc(file_path):\n",
        "    y, sr = librosa.load(file_path, sr=None)  # تحميل الملف الصوتي\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # استخراج 13 معامل MFCC\n",
        "    delta = librosa.feature.delta(mfcc)  # حساب المشتقة الأولى (لتتبع التغيرات)\n",
        "    delta2 = librosa.feature.delta(mfcc, order=2)  # المشتقة الثانية (لتسارع التغيرات)\n",
        "    return np.concatenate([np.mean(mfcc.T, axis=0), np.mean(delta.T, axis=0), np.mean(delta2.T, axis=0)])  # دمج الميزات"
      ],
      "metadata": {
        "id": "oRehI_hEHuO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_snr(y):\n",
        "    signal_power = np.mean(y ** 2)  # قوة الإشارة\n",
        "    noise = y - librosa.effects.remix([1], intervals=librosa.effects.split(y))  # عزل الضوضاء\n",
        "    noise_power = np.mean(noise ** 2) if len(noise) > 0 else 1e-9  # قوة الضوضاء\n",
        "    return 10 * np.log10((signal_power + 1e-9) / (noise_power + 1e-9))  # SNR بالديسيبل"
      ],
      "metadata": {
        "id": "cyvjUIHGHuVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_speech_vad(y, sr):\n",
        "    if sr != 16000:\n",
        "        y = librosa.resample(y, orig_sr=sr, target_sr=16000)  # تحويل معدل العينة إلى 16kHz\n",
        "    y_pcm = (y * 32767).astype(np.int16).tobytes()  # تحويل الصوت إلى تنسيق PCM\n",
        "    frame_size = int(16000 * 0.03) * 2  # حجم الإطار (30ms)\n",
        "    num_frames = len(y_pcm) // frame_size\n",
        "    speech_frames = sum(vad.is_speech(y_pcm[i*frame_size:(i+1)*frame_size], 16000) for i in range(num_frames))\n",
        "    return speech_frames > 0.4 * num_frames  # إذا كان 40% من الإطارات صوت بشري"
      ],
      "metadata": {
        "id": "mul3Set4HucL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_all_data(root_path, snr_threshold=15):\n",
        "    features, labels, langs = [], [], []\n",
        "    # Added a check to ensure dir_name is a directory\n",
        "    for dir_name in os.listdir(root_path):\n",
        "        dir_path = os.path.join(root_path, dir_name)\n",
        "        if os.path.isdir(dir_path):  # Check if it's a directory\n",
        "            tsv_path = os.path.join(dir_path, \"validated.tsv\")  # ملف البيانات\n",
        "            clips_path = os.path.join(dir_path, \"clips\")  # ملفات الصوت\n",
        "            # Also check if the required files/directories exist within the language folder\n",
        "            if os.path.exists(tsv_path) and os.path.exists(clips_path):\n",
        "                print(f\"Processing language directory: {dir_name}\")\n",
        "                try:\n",
        "                    df = pd.read_csv(tsv_path, sep=\"\\t\")  # قراءة البيانات\n",
        "                    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {dir_name} clips\"):\n",
        "                        # Added a check if the 'path' column exists and if the file exists\n",
        "                        if 'path' in row and isinstance(row['path'], str):\n",
        "                             file_path = os.path.join(clips_path, row[\"path\"])\n",
        "                             if os.path.exists(file_path):\n",
        "                                y, sr = sf.read(file_path)  # قراءة الصوت\n",
        "                                if not is_speech_vad(y, sr):  # تخطي إذا لم يكن صوتًا بشريًا\n",
        "                                    continue\n",
        "                                try:\n",
        "                                    snr = compute_snr(y)  # حساب SNR\n",
        "                                    label = 0 if snr >= snr_threshold else 1  # 0=نظيف، 1=مزعج\n",
        "                                    mfcc = extract_mfcc(file_path)  # استخراج الميزات\n",
        "                                    features.append(mfcc)\n",
        "                                    labels.append(label)\n",
        "                                except Exception as e:\n",
        "                                    print(f\"Error processing file {file_path}: {e}\")\n",
        "                             else:\n",
        "                                 print(f\"Clip file not found: {file_path}\")\n",
        "                        else:\n",
        "                             print(f\"Invalid or missing 'path' in row for {dir_name}\")\n",
        "                except FileNotFoundError:\n",
        "                    print(f\"Validated TSV not found in {dir_path}\")\n",
        "                except pd.errors.EmptyDataError:\n",
        "                     print(f\"Validated TSV is empty in {dir_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing directory {dir_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"Skipping {dir_path}: Missing validated.tsv or clips directory.\")\n",
        "        else:\n",
        "            # Print a message for non-directory entries found in the root path\n",
        "            print(f\"Skipping non-directory entry: {dir_name} found in {root_path}\")\n",
        "\n",
        "    # Add a check if any data was loaded\n",
        "    if not features:\n",
        "        print(\"No data loaded. Please check the dataset path and structure.\")\n",
        "\n",
        "    return np.array(features), np.array(labels)"
      ],
      "metadata": {
        "id": "afOy304HH5a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(X, y):\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Feature Scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Train XGBoost\n",
        "    clf = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=5,\n",
        "        eval_metric='logloss',\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_proba = clf.predict_proba(X_test)[:, 1]  # Probabilities for ROC-AUC\n",
        "\n",
        "    # --- Metrics ---\n",
        "    print(\"\\n Classification Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "    # Detailed Report\n",
        "    print(\"\\n Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Clean', 'Noisy']))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    print(\"\\n Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(confusion_matrix(y_test, y_pred),\n",
        "                annot=True, fmt='d',\n",
        "                cmap='Blues',\n",
        "                xticklabels=['Clean', 'Noisy'],\n",
        "                yticklabels=['Clean', 'Noisy'])\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FoquUrNKOTvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the content of ipython-input-9-972361fa1b80 with the following:\n",
        "\n",
        "def main():\n",
        "    # Extract the dataset\n",
        "    extract_dataset()\n",
        "\n",
        "    # Define the path to the extracted dataset root\n",
        "    # Assuming you want to process the English language data.\n",
        "    # Verify this path matches the actual extracted directory structure.\n",
        "    dataset_root = os.path.join(EXTRACT_TO, \"en\")\n",
        "    print(f\"Attempting to load data from: {dataset_root}\")\n",
        "\n",
        "    # Check if the dataset_root directory exists\n",
        "    if not os.path.exists(dataset_root):\n",
        "        print(f\"Error: Dataset root directory not found at {dataset_root}. Please check the extraction path and dataset structure.\")\n",
        "        # Exit or handle the error appropriately if the directory doesn't exist\n",
        "        return\n",
        "\n",
        "    # Load the data (features and labels)\n",
        "    features, labels = load_all_data(dataset_root, snr_threshold=SNR_THRESHOLD)\n",
        "\n",
        "    print(f\"Finished loading data. Number of samples loaded: {len(features)}\")\n",
        "\n",
        "    # Check if any data was loaded before training\n",
        "    if len(features) == 0:\n",
        "        print(\"No data was loaded after filtering. Cannot train the model.\")\n",
        "        print(\"Please check the dataset contents, the VAD filter, and the SNR threshold.\")\n",
        "    else:\n",
        "        # Train and evaluate the model\n",
        "        train_and_evaluate(features, labels)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41Db-YQG-eqF",
        "outputId": "827c85aa-e1bb-45f8-83c6-3fbbea71435a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Dataset file not found at /content/cv-corpus-19.0-delta-2024-09-13-en.tar.gz\n",
            "Attempting to load data from: /content/cv-corpus-19.0-delta-2024-09-13/en\n",
            "Error: Dataset root directory not found at /content/cv-corpus-19.0-delta-2024-09-13/en. Please check the extraction path and dataset structure.\n"
          ]
        }
      ]
    }
  ]
}